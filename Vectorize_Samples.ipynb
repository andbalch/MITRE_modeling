{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd09e45ca6d2f9a632adb471d6a7900c6b597dba7cec4c13cfc3bc8479688782778",
   "display_name": "Python 3.7.7 64-bit ('MITRE_modeling': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Vectorization of Samples\n",
    "To conserve resources and prepare the samples for final ingest, the last thing we need to do is turn all of the samples into vectors. This will be done using a custom tokenizer similar to that in tokenization.ipynb in MalDroid_feature_engineering repo. It will take the sample.apk.json files as a string and convert them to index ints. Due to the large size of our training dataset and the potential for the future computation of additional n-grams, performance and reusability will be optimized. The vocabulary.txt file from the aforementioned repo has been copied to local.\n",
    "## Algorithm Process (for sample in samples; given: regex delimiter from vocab)\n",
    "1. Fetch sample and load as dict, extract list of behaviors\n",
    "2. Call multisort function, ordering by id then ts \n",
    "3. For behavior in sorted list, drop unused keys\n",
    "4. Cast each dict in sorted list to string and add to str list of behaviors\n",
    "5. Initalize vector with SOA index, For behavior in list, call tokenizer function passing vector: \n",
    "6. Append vector with SOB index, init prev_matchEnd=0\n",
    "7. For match in finditer(delim, behavior):\n",
    "8. If match.start() - prev_matchEnd > 2, append vector with UNK index\n",
    "9. Append vector with match.lastgroup set prev_matchEnd to match.end(), next match\n",
    "10. Append vector with EOB index, return vector, next behavior\n",
    "11. Append vector with EOA index, save to .npy under *class*/*hash*.npy, next sample\n",
    "## Current Objectives\n",
    "1. Fetch a random array of samples. This is intended to account for having to cut down the sample size while minimizing any bias therein. Needs to be done with a random seed so array is the same on each run, making it robust. Additionally, samples need to be interleaved by class so no one class is overrepresented. \n",
    "2. Implement resumable infacture. This may be in the form of a 'last_token' variable outside of the main loop so it can be fetched if the loop crashes. Sometimes if a large script crashes the kernel requires a restart, clearing all variables. Therefore, this variable needs to be written to a file after each run.\n",
    "3. Introduce multithreading with the multiprocessing library if possible. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import timeit\n",
    "from statistics import mean\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_behaviors(raw_behaviors):\n",
    "    raw_behaviors.sort(key = lambda x: x['low'][0]['id'])\n",
    "    raw_behaviors.sort(key = lambda x: float(x['low'][0]['ts']))\n",
    "    return raw_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_unused_keys(behavior_list, unused_keys):\n",
    "    behavior_index = 0\n",
    "    for behavior in behavior_list:\n",
    "        behavior = {key: value for key, value in behavior.items() if key not in unused_keys}\n",
    "        sub_behavior_index = 0\n",
    "        for sub_behavior in behavior['low']:\n",
    "            sub_behavior = {sub_key: sub_value for sub_key, sub_value in sub_behavior.items() if sub_key not in unused_keys}\n",
    "            behavior['low'][sub_behavior_index] = sub_behavior\n",
    "            sub_behavior_index += 1\n",
    "        behavior_list[behavior_index] = behavior\n",
    "        behavior_index += 1\n",
    "    return behavior_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(behavior, delimiter):\n",
    "    behavior_vector = [3]\n",
    "    prev_matchEnd = 0\n",
    "\n",
    "    for match in re.finditer(delimiter, behavior):\n",
    "        matchStart = match.start()\n",
    "        if (matchStart - prev_matchEnd) > 2 and any(char.isalnum() for char in behavior[prev_matchEnd:matchStart]) and 'low' not in behavior[prev_matchEnd:matchStart]: \n",
    "            behavior_vector.append(0)\n",
    "        behavior_vector.append(int(match.lastgroup[1:]))\n",
    "        prev_matchEnd = match.end()\n",
    "\n",
    "    behavior_vector.append(4)\n",
    "\n",
    "    return behavior_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sample, regex_pattern, parent_dir):\n",
    "    with open('resume_dependencies.txt', 'w') as write_resume:\n",
    "        write_resume.write('Current sample: \\n')\n",
    "        write_resume.write('{0} \\n'.format(sample))\n",
    "    sample_times = []\n",
    "    sample_hash, sample_class = sample\n",
    "    start_sample = time.time()\n",
    "    with open(parent_dir + sample_class + '\\\\' + sample_hash + '\\\\sample_for_analysis.apk.json') as sample_path:\n",
    "        try:\n",
    "            sample_behaviors = json.load(sample_path)['behaviors']['dynamic']['host']\n",
    "        except:\n",
    "            with open('error_hashes.txt', 'a+') as write_errors:\n",
    "                write_resume.write(sample_hash)\n",
    "            print(\"Error loading hash {0}\".format(sample_hash))\n",
    "            return None\n",
    "    load_end = time.time()\n",
    "\n",
    "    sort_start = time.time()\n",
    "    sorted_behaviors = sort_behaviors(sample_behaviors)\n",
    "    sort_end = time.time()\n",
    "    sample_behaviors = []\n",
    "    # lists are cleared after useage to preserve memory resources\n",
    "\n",
    "    strip_start = time.time()\n",
    "    stripped_behaviors = strip_unused_keys(sorted_behaviors, ['arguments', 'blob', 'parameters', 'id', 'xref', 'ts', 'tid', 'interfaceGroup', 'methodName'])\n",
    "    strip_end = time.time()\n",
    "    sorted_behaviors = []\n",
    "\n",
    "    cast_str_start = time.time()\n",
    "    string_behaviors = [json.dumps(behavior) for behavior in stripped_behaviors] \n",
    "    cast_str_end = time.time()\n",
    "    stripped_behaviors = []\n",
    "\n",
    "    vector_start = time.time()\n",
    "    token_times = []\n",
    "    append_times = []\n",
    "    vectorized_sample = [1]\n",
    "    for behavior in string_behaviors:\n",
    "        token_start = time.time()\n",
    "        append_to_vector = tokenize(behavior, regex_pattern)\n",
    "        token_end = time.time()\n",
    "        token_times.append(token_end-token_start)\n",
    "        append_start = time.time()\n",
    "        for scalar in append_to_vector:\n",
    "            vectorized_sample.append(scalar)\n",
    "        append_end = time.time()\n",
    "        append_times.append(append_end-append_start)\n",
    "    vectorized_sample.append(2)\n",
    "    vector_end = time.time()\n",
    "\n",
    "    write_start = time.time()\n",
    "    # with open(\"vectorized_samples/\" + sample_class + \"/\" + sample_hash + \".npy\", 'wb') as vector_path:\n",
    "    #     np.save(vector_path, vectorized_sample, allow_pickle = False)\n",
    "    end_sample = time.time()\n",
    "\n",
    "    print(\"full pass: {0}\".format(end_sample-start_sample))\n",
    "    print(\"load sample: {0}\".format(load_end-start_sample))\n",
    "    print(\"sort: {0}\".format(sort_end-sort_start))\n",
    "    print(\"strip: {0}\".format(strip_end-strip_start))\n",
    "    print(\"cast: {0}\".format(cast_str_end-cast_str_start))\n",
    "    print(\"vectorize: {0}\\n\".format(vector_end-vector_start))\n",
    "    print(\"avg tokenize: {0}\".format(mean(token_times)))\n",
    "    print(\"write: {0}\".format(end_sample-write_start))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_interleave(adware, banking, riskware, sms):\n",
    "    final_list = []\n",
    "\n",
    "    random.seed(42)\n",
    "    #sets random seed for shuffling of samples\n",
    "    random.shuffle(adware)\n",
    "    random.shuffle(banking)\n",
    "    random.shuffle(riskware)\n",
    "    random.shuffle(sms)\n",
    "\n",
    "    adware_len = len(adware)\n",
    "    banking_len = len(banking)\n",
    "    riskware_len = len(riskware)\n",
    "    sms_len = len(sms)\n",
    "\n",
    "    for index in range(max(adware_len, banking_len, riskware_len, sms_len)):\n",
    "        if index < adware_len:\n",
    "            final_list.append((adware[index], 'adware'))\n",
    "        if index < banking_len:\n",
    "            final_list.append((banking[index], 'banking'))\n",
    "        if index < riskware_len:\n",
    "            final_list.append((riskware[index], 'riskware'))\n",
    "        if index < sms_len:\n",
    "            final_list.append((sms[index], 'sms'))\n",
    "\n",
    "    return(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt') as vocab_path:\n",
    "    vocab_file = vocab_path.read()\n",
    "\n",
    "vocab = ast.literal_eval(vocab_file)\n",
    "\n",
    "token_specification = []\n",
    "delimiter_list = []\n",
    "\n",
    "for packed_items in vocab.items():\n",
    "    index, value = packed_items\n",
    "    delimiter, literal = value\n",
    "    if delimiter != None:\n",
    "        delimiter_list.append(str(delimiter))\n",
    "        token_specification.append(('I' + str(index), delimiter))\n",
    "\n",
    "regex_pattern = '|'.join('(?P<{0}>{1})'.format(name, delim) for name, delim in token_specification)\n",
    "# calling re.compile() on the pattern could lead to increased performance, but causes issues with backslashes, therefore it is omitted\n",
    "\n",
    "parent_dir = 'X:\\\\MITRE\\\\MalDroid Data\\\\MalDroid_feature_engineering\\\\'\n",
    "adware_hashes = [adware_hash for adware_hash in os.listdir(parent_dir + 'adware\\\\')]\n",
    "banking_hashes = [banking_hash for banking_hash in os.listdir(parent_dir + 'banking\\\\')]\n",
    "riskware_hashes = [riskware_hash for riskware_hash in os.listdir(parent_dir + 'riskware\\\\')]\n",
    "sms_hashes = [sms_hash for sms_hash in os.listdir(parent_dir + 'sms\\\\')]\n",
    "\n",
    "sample_list = shuffle_and_interleave(adware_hashes, banking_hashes, riskware_hashes, sms_hashes)\n",
    "\n",
    "current_sample = 'Start'\n",
    "error_hashes = []\n",
    "\n",
    "for sample in sample_list[:10]:\n",
    "    vectorize(sample, regex_pattern, parent_dir)"
   ]
  }
 ]
}