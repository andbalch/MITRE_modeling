{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('MITRE_modeling': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9e45ca6d2f9a632adb471d6a7900c6b597dba7cec4c13cfc3bc8479688782778"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Vectorization of Samples\n",
    "To conserve resources and prepare the samples for final ingest, the last thing we need to do is turn all of the samples into vectors. This will be done using a custom tokenizer similar to that in tokenization.ipynb in MalDroid_feature_engineering repo. It will take the sample.apk.json files as a string and convert them to index ints. Due to the large size of our training dataset and the potential for the future computation of additional n-grams, performance and reusability will be optimized. The vocabulary.txt file from the aforementioned repo has been copied to local.\n",
    "## Algorithm Process (for sample in samples; given: regex delimiter from vocab)\n",
    "1. Fetch sample and load as dict, extract list of behaviors\n",
    "2. Call multisort function, ordering by id then ts \n",
    "3. For behavior in sorted list, drop unused keys\n",
    "4. Cast each dict in sorted list to string and add to str list of behaviors\n",
    "5. Initalize vector with SOA index, For behavior in list, call tokenizer function passing vector: \n",
    "6. Append vector with SOB index, init prev_matchEnd=0\n",
    "7. For match in finditer(delim, behavior):\n",
    "8. If match.start() - prev_matchEnd > 2, append vector with UNK index\n",
    "9. Append vector with match.lastgroup set prev_matchEnd to match.end(), next match\n",
    "10. Append vector with EOB index, return vector, next behavior\n",
    "11. Append vector with EOA index, save to .tfrecord under *class*/*hash*.tfrecord, next sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import timeit\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt') as vocab_path:\n",
    "    vocab_file = vocab_path.read()\n",
    "\n",
    "vocab = ast.literal_eval(vocab_file)\n",
    "\n",
    "token_specification = []\n",
    "delimiter_list = []\n",
    "\n",
    "for packed_items in vocab.items():\n",
    "    index, value = packed_items\n",
    "    delimiter, literal = value\n",
    "    if delimiter != None:\n",
    "        delimiter_list.append(str(delimiter))\n",
    "        token_specification.append(('I' + str(index), delimiter))\n",
    "\n",
    "regex_pattern = '|'.join('(?P<{0}>{1})'.format(name, delim) for name, delim in token_specification)\n",
    "# calling re.compile() on the pattern could lead to increased performance, but causes issues with backslashes, therefore it is omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_behaviors(raw_behaviors):\n",
    "    raw_behaviors.sort(key = lambda x: x['low'][0]['id'])\n",
    "    raw_behaviors.sort(key = lambda x: float(x['low'][0]['ts']))\n",
    "    return raw_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_unused_keys(behavior_list, unused_keys):\n",
    "    behavior_index = 0\n",
    "    for behavior in behavior_list:\n",
    "        behavior = {key: value for key, value in behavior.items() if key not in unused_keys}\n",
    "        sub_behavior_index = 0\n",
    "        for sub_behavior in behavior['low']:\n",
    "            sub_behavior = {sub_key: sub_value for sub_key, sub_value in sub_behavior.items() if sub_key not in unused_keys}\n",
    "            behavior['low'][sub_behavior_index] = sub_behavior\n",
    "            sub_behavior_index += 1\n",
    "        behavior_list[behavior_index] = behavior\n",
    "        behavior_index += 1\n",
    "    return behavior_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(behavior, delimiter):\n",
    "    behavior_vector = [3]\n",
    "    prev_matchEnd = 0\n",
    "\n",
    "    for match in re.finditer(delimiter, behavior):\n",
    "        if (match.start() - prev_matchEnd) > 2: \n",
    "            behavior_vector.append(0)\n",
    "        behavior_vector.append(int(match.lastgroup[1:]))\n",
    "        prev_matchEnd = match.end()\n",
    "    behavior_vector.append(4)\n",
    "    \n",
    "    return behavior_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.08800029754638672\n",
      "0.025996685028076172\n",
      "0.15400099754333496\n",
      "0.037000179290771484\n",
      "0.027999401092529297\n",
      "0.026000022888183594\n",
      "0.027001380920410156\n",
      "0.027001142501831055\n",
      "0.02700018882751465\n",
      "0.026999473571777344\n",
      "0.02700042724609375\n",
      "0.029006004333496094\n",
      "0.026993274688720703\n",
      "0.0280001163482666\n",
      "0.026006221771240234\n",
      "0.027994871139526367\n",
      "0.02700185775756836\n",
      "0.0279998779296875\n",
      "0.027997732162475586\n",
      "0.03000020980834961\n",
      "0.026999473571777344\n",
      "0.027000904083251953\n",
      "0.027999162673950195\n",
      "0.03400111198425293\n",
      "0.029007911682128906\n",
      "0.027991771697998047\n",
      "0.02700018882751465\n",
      "0.029000282287597656\n",
      "0.02700018882751465\n",
      "0.027998685836791992\n",
      "0.030002593994140625\n",
      "0.033997535705566406\n",
      "0.03299975395202637\n",
      "0.032001495361328125\n",
      "0.030005693435668945\n",
      "0.033006906509399414\n",
      "0.028993844985961914\n",
      "0.032998085021972656\n",
      "0.029999732971191406\n",
      "0.02899956703186035\n",
      "0.029007911682128906\n",
      "0.028995275497436523\n",
      "0.027997732162475586\n",
      "0.028001070022583008\n",
      "0.029004573822021484\n",
      "0.02899479866027832\n",
      "0.0279996395111084\n",
      "0.031000852584838867\n",
      "0.029004812240600586\n",
      "0.02799701690673828\n",
      "0.027999162673950195\n",
      "0.028998374938964844\n",
      "0.029003620147705078\n",
      "0.031001567840576172\n",
      "0.028995990753173828\n",
      "0.03099966049194336\n",
      "0.03000187873840332\n",
      "0.027999162673950195\n",
      "0.03299975395202637\n",
      "0.03199887275695801\n",
      "0.028001070022583008\n",
      "0.031999826431274414\n",
      "0.029999256134033203\n",
      "0.026001453399658203\n",
      "0.026999711990356445\n",
      "0.029000043869018555\n",
      "0.027000904083251953\n",
      "0.02799844741821289\n",
      "0.026999950408935547\n",
      "0.02700018882751465\n",
      "0.027999401092529297\n",
      "0.026999711990356445\n",
      "0.02699732780456543\n",
      "0.02599930763244629\n",
      "0.026999473571777344\n",
      "0.026000022888183594\n",
      "0.029000520706176758\n",
      "0.02700018882751465\n",
      "0.0279996395111084\n",
      "0.026999473571777344\n",
      "0.02700042724609375\n",
      "0.026999950408935547\n",
      "0.03800225257873535\n",
      "0.024997234344482422\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0fb2ed2dc25a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbehavior\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_behaviors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mtoken_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mappend_to_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbehavior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex_pattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mtoken_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_end\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtoken_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-32330e73893b>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(behavior, delimiter)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprev_matchEnd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mbehavior_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mbehavior_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlastgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mal_classes = ['adware', 'banking', 'riskware', 'sms']\n",
    "error_hashes = []\n",
    "parent_dir = 'X:\\\\MITRE\\\\MalDroid Data\\\\MalDroid_feature_engineering\\\\'\n",
    "\n",
    "for mal_class in mal_classes:\n",
    "    for sample_folder in os.listdir(parent_dir + mal_class + '\\\\'):\n",
    "        # with open(parent_dir + mal_class + '\\\\' + sample_folder + '\\\\sample_for_analysis.apk.json') as sample_path:\n",
    "        start_sample = time.time()\n",
    "        with open('X:\\\\MITRE\\\\MalDroid Data\\\\MalDroid_feature_engineering\\\\riskware\\\\f6036a5730f8ec961bc1666cfc3269dbf9b854b1a705a65610154a697e804d79\\\\sample_for_analysis.apk.json') as sample_path:\n",
    "            try:\n",
    "                sample_behaviors = json.load(sample_path)['behaviors']['dynamic']['host']\n",
    "            except:\n",
    "                error_hashes.append(sample_folder)\n",
    "                continue\n",
    "        load_end = time.time()\n",
    "\n",
    "        sort_start = time.time()\n",
    "        sorted_behaviors = sort_behaviors(sample_behaviors)\n",
    "        sort_end = time.time()\n",
    "        sample_behaviors = []\n",
    "        # lists are cleared after useage to preserve memory resources\n",
    "\n",
    "        strip_start = time.time()\n",
    "        stripped_behaviors = strip_unused_keys(sorted_behaviors, [])\n",
    "        strip_end = time.time()\n",
    "        sorted_behaviors = []\n",
    "\n",
    "        cast_str_start = time.time()\n",
    "        string_behaviors = [json.dumps(behavior) for behavior in stripped_behaviors] \n",
    "        cast_str_end = time.time()\n",
    "        # check to ensure this preserves json syntax (ex \"\" not '' in keys)\n",
    "        stripped_behaviors = []\n",
    "\n",
    "        vector_start = time.time()\n",
    "        token_times = []\n",
    "        append_times = []\n",
    "        vectorized_sample = [1]\n",
    "        for behavior in string_behaviors:\n",
    "            token_start = time.time()\n",
    "            append_to_vector = tokenize(behavior, regex_pattern)\n",
    "            token_end = time.time()\n",
    "            token_times.append(token_end-token_start)\n",
    "            append_start = time.time()\n",
    "            for scalar in append_to_vector:\n",
    "                vectorized_sample.append(scalar)\n",
    "            append_end = time.time()\n",
    "            append_times.append(append_end-append_start)\n",
    "        vectorized_sample.append(2)\n",
    "        vector_end = time.time()\n",
    "\n",
    "        ds_start = time.time()\n",
    "        tf_dataset = tf.data.Dataset.from_tensor_slices(vectorized_sample)\n",
    "        tf_dataset = tf_dataset.map(tf.io.serialize_tensor)\n",
    "        ds_end = time.time()\n",
    "\n",
    "        write_start = time.time()\n",
    "        writer = tf.data.experimental.TFRecordWriter(\"TFRecord_files/\" + mal_class + \"/\" + sample_folder + \".tfrecord\")\n",
    "        writer.write(tf_dataset)\n",
    "        end_sample = time.time()\n",
    "\n",
    "        print(\"full pass: {0}\".format(end_sample-start_sample))\n",
    "        print(\"load sample: {0}\".format(load_end-start_sample))\n",
    "        print(\"sort: {0}\".format(sort_end-sort_start))\n",
    "        print(\"strip: {0}\".format(strip_end-strip_start))\n",
    "        print(\"cast: {0}\".format(cast_str_end-cast_str_start))\n",
    "        print(\"vectorize: {0}\".format(vector_end-vector_start))\n",
    "        print(\"avg tokenize: {0}\".format(mean(token_times)))\n",
    "        print(\"avg append: {0}\".format(mean(append_times)))\n",
    "        print(\"dataset: {0}\".format(ds_end-ds_start))\n",
    "        print(\"write: {0}\".format(write_end-write_start))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}