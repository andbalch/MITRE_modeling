{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd09e45ca6d2f9a632adb471d6a7900c6b597dba7cec4c13cfc3bc8479688782778",
   "display_name": "Python 3.7.7 64-bit ('MITRE_modeling': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Vectorization of Samples\n",
    "To conserve resources and prepare the samples for final ingest, the last thing we need to do is turn all of the samples into vectors. This will be done using a custom tokenizer similar to that in tokenization.ipynb in MalDroid_feature_engineering repo. It will take the sample.apk.json files as a string and convert them to index ints. Due to the large size of our training dataset and the potential for the future computation of additional n-grams, performance and reusability will be optimized. The vocabulary.txt file from the aforementioned repo has been copied to local.\n",
    "## Algorithm Process (for sample in samples; given: regex delimiter from vocab)\n",
    "1. Fetch sample and load as dict, extract list of behaviors\n",
    "2. Call multisort function, ordering by id then ts \n",
    "3. For behavior in sorted list, drop unused keys\n",
    "4. Cast each dict in sorted list to string and add to str list of behaviors\n",
    "5. Initalize vector with SOA index, For behavior in list, call tokenizer function passing vector: \n",
    "6. Append vector with SOB index, init prev_matchEnd=0\n",
    "7. For match in finditer(delim, behavior):\n",
    "8. If match.start() - prev_matchEnd > 2, append vector with UNK index\n",
    "9. Append vector with match.lastgroup set prev_matchEnd to match.end(), next match\n",
    "10. Append vector with EOB index, return vector, next behavior\n",
    "11. Append vector with EOA index, save to .npy under *class*/*hash*.npy, next sample\n",
    "## Current Objectives\n",
    "1. Verify sorting algo works as intended\n",
    "2. Check tokenizer similar to method in tokenizer.ipynb\n",
    "3. Cleanup code\n",
    "4. Implement some form of a progress bar"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "from statistics import mean\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import sys\n",
    "from functools import partial\n",
    "import imported_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_behaviors(raw_behaviors):\n",
    "    raw_behaviors.sort(key = lambda x: x['low'][0]['id'])\n",
    "    raw_behaviors.sort(key = lambda x: float(x['low'][0]['ts']))\n",
    "    return raw_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_unused_keys(behavior_list, unused_keys):\n",
    "    behavior_index = 0\n",
    "    for behavior in behavior_list:\n",
    "        behavior = {key: value for key, value in behavior.items() if key not in unused_keys}\n",
    "        sub_behavior_index = 0\n",
    "        for sub_behavior in behavior['low']:\n",
    "            sub_behavior = {sub_key: sub_value for sub_key, sub_value in sub_behavior.items() if sub_key not in unused_keys}\n",
    "            behavior['low'][sub_behavior_index] = sub_behavior\n",
    "            sub_behavior_index += 1\n",
    "        behavior_list[behavior_index] = behavior\n",
    "        behavior_index += 1\n",
    "    return behavior_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(behavior, delimiter):\n",
    "    behavior_vector = [3]\n",
    "    prev_matchEnd = 0\n",
    "\n",
    "    for match in re.finditer(delimiter, behavior):\n",
    "        matchStart = match.start()\n",
    "        if (matchStart - prev_matchEnd) > 2 and any(char.isalnum() for char in behavior[prev_matchEnd:matchStart]) and 'low' not in behavior[prev_matchEnd:matchStart]: \n",
    "            behavior_vector.append(0)\n",
    "        behavior_vector.append(int(match.lastgroup[1:]))\n",
    "        prev_matchEnd = match.end()\n",
    "\n",
    "    behavior_vector.append(4)\n",
    "\n",
    "    return behavior_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sample, regex_pattern, parent_dir):\n",
    "    with open('resume_dependencies.txt', 'w') as write_resume:\n",
    "        write_resume.write('Current sample: \\n')\n",
    "        write_resume.write('{0} \\n'.format(sample))\n",
    "    sample_hash, sample_class = sample\n",
    "    with open(parent_dir + sample_class + '\\\\' + sample_hash + '\\\\sample_for_analysis.apk.json') as sample_path:\n",
    "        try:\n",
    "            sample_behaviors = json.load(sample_path)['behaviors']['dynamic']['host']\n",
    "        except:\n",
    "            with open('error_hashes.txt', 'a+') as write_errors:\n",
    "                write_resume.write(sample_hash)\n",
    "            print(\"Error loading hash {0}\".format(sample_hash))\n",
    "            return None\n",
    "    sorted_behaviors = sort_behaviors(sample_behaviors)\n",
    "    sample_behaviors = []\n",
    "    # lists are cleared after useage to preserve memory resources\n",
    "\n",
    "    stripped_behaviors = strip_unused_keys(sorted_behaviors, ['arguments', 'blob', 'parameters', 'id', 'xref', 'ts', 'tid', 'interfaceGroup', 'methodName'])\n",
    "    sorted_behaviors = []\n",
    "\n",
    "    string_behaviors = [json.dumps(behavior) for behavior in stripped_behaviors] \n",
    "    stripped_behaviors = []\n",
    "\n",
    "    vectorized_sample = [1]\n",
    "    for behavior in string_behaviors:\n",
    "        append_to_vector = tokenize(behavior, regex_pattern)\n",
    "        for scalar in append_to_vector:\n",
    "            vectorized_sample.append(scalar)\n",
    "    vectorized_sample.append(2)\n",
    "\n",
    "    with open(\"vectorized_samples/\" + sample_class + \"/\" + sample_hash + \".npy\", 'wb') as vector_path:\n",
    "        np.save(vector_path, vectorized_sample, allow_pickle = False)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_interleave(adware, banking, riskware, sms):\n",
    "    final_list = []\n",
    "\n",
    "    random.seed(42)\n",
    "    #sets random seed for shuffling of samples\n",
    "    random.shuffle(adware)\n",
    "    random.shuffle(banking)\n",
    "    random.shuffle(riskware)\n",
    "    random.shuffle(sms)\n",
    "\n",
    "    adware_len = len(adware)\n",
    "    banking_len = len(banking)\n",
    "    riskware_len = len(riskware)\n",
    "    sms_len = len(sms)\n",
    "\n",
    "    for index in range(max(adware_len, banking_len, riskware_len, sms_len)):\n",
    "        if index < adware_len:\n",
    "            final_list.append((adware[index], 'adware'))\n",
    "        if index < banking_len:\n",
    "            final_list.append((banking[index], 'banking'))\n",
    "        if index < riskware_len:\n",
    "            final_list.append((riskware[index], 'riskware'))\n",
    "        if index < sms_len:\n",
    "            final_list.append((sms[index], 'sms'))\n",
    "\n",
    "    return(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt') as vocab_path:\n",
    "    vocab_file = vocab_path.read()\n",
    "\n",
    "vocab = ast.literal_eval(vocab_file)\n",
    "\n",
    "token_specification = []\n",
    "delimiter_list = []\n",
    "\n",
    "for packed_items in vocab.items():\n",
    "    index, value = packed_items\n",
    "    delimiter, literal = value\n",
    "    if delimiter != None:\n",
    "        delimiter_list.append(str(delimiter))\n",
    "        token_specification.append(('I' + str(index), delimiter))\n",
    "\n",
    "regex_pattern = '|'.join('(?P<{0}>{1})'.format(name, delim) for name, delim in token_specification)\n",
    "# calling re.compile() on the pattern could lead to increased performance, but causes issues with backslashes, therefore it is omitted\n",
    "\n",
    "parent_dir = 'X:\\\\MITRE\\\\MalDroid Data\\\\MalDroid_feature_engineering\\\\'\n",
    "adware_hashes = [adware_hash for adware_hash in os.listdir(parent_dir + 'adware\\\\')]\n",
    "banking_hashes = [banking_hash for banking_hash in os.listdir(parent_dir + 'banking\\\\')]\n",
    "riskware_hashes = [riskware_hash for riskware_hash in os.listdir(parent_dir + 'riskware\\\\')]\n",
    "sms_hashes = [sms_hash for sms_hash in os.listdir(parent_dir + 'sms\\\\')]\n",
    "\n",
    "sample_list = shuffle_and_interleave(adware_hashes, banking_hashes, riskware_hashes, sms_hashes)\n",
    "\n",
    "# Below code sets up the process to resume from the last processed hash\n",
    "resume_index = sample_list.index(('0588209d36ec742b4763f10af61ae6db31314cac55f8f2d6141f8b6f3f187faf', 'riskware'))\n",
    "\n",
    "# Below code is for non-multithreaded processes\n",
    "# for sample in tqdm(sample_list[1:10]):\n",
    "#     vectorize(sample, regex_pattern, parent_dir)\n",
    "\n",
    "# Below code is for multithreaded processes w/o pool\n",
    "# multi_start = time.time()\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# inputs = sample_list[:20]\n",
    "# if __name__ == \"__main__\":\n",
    "#     result = Parallel(n_jobs=num_cores)(delayed(vectorize)(i, regex_pattern, parent_dir) for i in inputs)\n",
    "\n",
    "# Below code is for multithreaded processes w pool and starmap, uses imported version of the vectorizer and its dependencies\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "inputs = [(i, regex_pattern, parent_dir) for i in sample_list[resume_index:]]\n",
    "if __name__ == \"__main__\":\n",
    "    with multiprocessing.Pool(num_cores) as pool:\n",
    "        result = pool.starmap(imported_vectorize.vectorize, inputs)\n",
    "\n",
    "# Below code is for multithreaded processes w pool and map_async, uses imported version of the vectorizer and its dependencies\n",
    "# multi_start = time.time()\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# inputs = sample_list[:20]\n",
    "# partial_call = partial(imported_vectorize.vectorize, regex_pattern=regex_pattern, parent_dir=parent_dir)\n",
    "# if __name__ == \"__main__\":\n",
    "#     with multiprocessing.Pool(num_cores) as pool:\n",
    "#         result = pool.map_async(partial_call, inputs)\n",
    "#         result.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}