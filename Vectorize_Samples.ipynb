{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('MITRE_modeling': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9e45ca6d2f9a632adb471d6a7900c6b597dba7cec4c13cfc3bc8479688782778"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Vectorization of Samples\n",
    "To conserve resources and prepare the samples for final ingest, the last thing we need to do is turn all of the samples into vectors. This will be done using a custom tokenizer similar to that in tokenization.ipynb in MalDroid_feature_engineering repo. It will take the sample.apk.json files as a string and convert them to index ints. Due to the large size of our training dataset and the potential for the future computation of additional n-grams, performance and reusability will be optimized. The vocabulary.txt file from the aforementioned repo has been copied to local.\n",
    "## Algorithm Process (for sample in samples; given: regex delimiter from vocab)\n",
    "1. Fetch sample and load as dict, extract list of behaviors\n",
    "2. Call multisort function, ordering by id then ts \n",
    "3. For behavior in sorted list, drop unused keys\n",
    "4. Cast each dict in sorted list to string and add to str list of behaviors\n",
    "5. Initalize vector with SOA index, For behavior in list, call tokenizer function passing vector: \n",
    "6. Append vector with SOB index, init prev_matchEnd=0\n",
    "7. For match in finditer(delim, behavior):\n",
    "8. If match.start() - prev_matchEnd > 2, append vector with UNK index\n",
    "9. Append vector with match.lastgroup set prev_matchEnd to match.end(), next match\n",
    "10. Append vector with EOB index, return vector, next behavior\n",
    "11. Append vector with EOA index, save to .tfrecord under *class*/*hash*.tfrecord, next sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_behaviors(raw_behaviors):\n",
    "    raw_behaviors.sort(key = lambda x: x['low'][0]['id'])\n",
    "    raw_behaviors.sort(key = lambda x: float(x['low'][0]['ts']))\n",
    "    #check to ensure this correctly parses str to float\n",
    "    return raw_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_unused_keys(behavior_list, unused_keys):\n",
    "    behavior_index = 0\n",
    "    for behavior in behavior_list:\n",
    "        behavior = {key: value for key, value in behavior.items() if key not in unused_keys}\n",
    "        sub_behavior_index = 0\n",
    "        for sub_behavior in behavior['low']:\n",
    "            sub_behavior = {sub_key: sub_value for sub_key, sub_value in sub_behavior.items() if sub_key not in unused_keys}\n",
    "            behavior['low'][sub_behavior_index] = sub_behavior\n",
    "            sub_behavior_index += 1\n",
    "        behavior_list[behavior_index] = behavior\n",
    "        behavior_index += 1\n",
    "    return behavior_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(vector, behavior):\n",
    "    vector = np.array(vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_classes = ['adware', 'banking', 'riskware', 'sms']\n",
    "error_hashes = []\n",
    "parent_dir = 'X:\\\\MITRE\\\\MalDroid Data\\\\MalDroid_feature_engineering\\\\'\n",
    "\n",
    "for mal_class in mal_classes:\n",
    "    for sample_folder in os.listdir(parent_dir + mal_class + '\\\\'):\n",
    "        with open(parent_dir + mal_class + '\\\\' + sample_folder + '\\\\sample_for_analysis.apk.json') as sample_path:\n",
    "            try:\n",
    "                sample_behaviors = json.loads(sample_path)['behaviors']['dynamic']['host']\n",
    "            except:\n",
    "                error_hashes.append(sample_folder)\n",
    "                sample_behaviors *= 0 \n",
    "                continue\n",
    "\n",
    "        sorted_behaviors = sort_behaviors(sample_behaviors)\n",
    "        sample_behaviors *= 0 \n",
    "        # lists are cleared after useage to preserve memory resources\n",
    "\n",
    "        stripped_behaviors = strip_unused_keys(sorted_behaviors, [])\n",
    "        sorted_behaviors *= 0 \n",
    "\n",
    "        string_behaviors = [for ast.literal_eval(behavior) in stripped_behaviors] \n",
    "        # check to ensure this preserves json syntax (ex \"\" not '' in keys)\n",
    "        stripped_behaviors *= 0 \n",
    "\n",
    "        vectorized_sample = [1]\n",
    "        for behavior in string_behaviors:\n",
    "            np.concatenate([vectorized_sample, tokenize(vectorized_sample, behavior)])"
   ]
  }
 ]
}