{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('MITRE_modeling': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9e45ca6d2f9a632adb471d6a7900c6b597dba7cec4c13cfc3bc8479688782778"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Vectorization of Samples\n",
    "To conserve resources and prepare the samples for final ingest, the last thing we need to do is turn all of the samples into vectors. This will be done using a custom tokenizer similar to that in tokenization.ipynb in MalDroid_feature_engineering repo. It will take the sample.apk.json files as a string and convert them to index ints. Due to the large size of our training dataset and the potential for the future computation of additional n-grams, performance and reusability will be optimized. The vocabulary.txt file from the aforementioned repo has been copied to local.\n",
    "## Algorithm Process (for sample in samples; given: regex delimiter from vocab)\n",
    "1. Fetch sample and load as dict, extract list of behaviors\n",
    "2. Call multisort function, ordering by id then ts \n",
    "3. For behavior in sorted list, drop unused keys\n",
    "4. Cast each dict in sorted list to string and add to str list of behaviors\n",
    "5. Initalize vector with SOA index, For behavior in list, call tokenizer function passing vector: \n",
    "6. Append vector with SOB index, init prev_matchEnd=0\n",
    "7. For match in finditer(delim, behavior):\n",
    "8. If match.start() - prev_matchEnd > 2, append vector with UNK index\n",
    "9. Append vector with match.lastgroup set prev_matchEnd to match.end(), next match\n",
    "10. Append vector with EOB index, return vector, next behavior\n",
    "11. Append vector with EOA index, save to .tfrecord under *class*/*hash*.tfrecord, next sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import timeit\n",
    "from statistics import mean\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt') as vocab_path:\n",
    "    vocab_file = vocab_path.read()\n",
    "\n",
    "vocab = ast.literal_eval(vocab_file)\n",
    "\n",
    "token_specification = []\n",
    "delimiter_list = []\n",
    "\n",
    "for packed_items in vocab.items():\n",
    "    index, value = packed_items\n",
    "    delimiter, literal = value\n",
    "    if delimiter != None:\n",
    "        delimiter_list.append(str(delimiter))\n",
    "        token_specification.append(('I' + str(index), delimiter))\n",
    "\n",
    "regex_pattern = '|'.join('(?P<{0}>{1})'.format(name, delim) for name, delim in token_specification)\n",
    "# calling re.compile() on the pattern could lead to increased performance, but causes issues with backslashes, therefore it is omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_behaviors(raw_behaviors):\n",
    "    raw_behaviors.sort(key = lambda x: x['low'][0]['id'])\n",
    "    raw_behaviors.sort(key = lambda x: float(x['low'][0]['ts']))\n",
    "    return raw_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_unused_keys(behavior_list, unused_keys):\n",
    "    behavior_index = 0\n",
    "    for behavior in behavior_list:\n",
    "        behavior = {key: value for key, value in behavior.items() if key not in unused_keys}\n",
    "        sub_behavior_index = 0\n",
    "        for sub_behavior in behavior['low']:\n",
    "            sub_behavior = {sub_key: sub_value for sub_key, sub_value in sub_behavior.items() if sub_key not in unused_keys}\n",
    "            behavior['low'][sub_behavior_index] = sub_behavior\n",
    "            sub_behavior_index += 1\n",
    "        behavior_list[behavior_index] = behavior\n",
    "        behavior_index += 1\n",
    "    return behavior_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(behavior, delimiter):\n",
    "    behavior_vector = [3]\n",
    "    prev_matchEnd = 0\n",
    "\n",
    "    for match in re.finditer(delimiter, behavior):\n",
    "        matchStart = match.start()\n",
    "        if (prev_matchEnd - matchStart) > 2 and (\"low\" not in behavior[prev_matchEnd:matchStart]): \n",
    "            behavior_vector.append(0)\n",
    "            print(behavior[prev_matchEnd:matchStart])\n",
    "        behavior_vector.append(int(match.lastgroup[1:]))\n",
    "        prev_matchEnd = match.end()\n",
    "    behavior_vector.append(4)\n",
    "\n",
    "    return behavior_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-c7675d8ca582>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"tokenize\" failed type inference due to: \u001b[1m\u001b[1mUnknown attribute 'finditer' of type Module(<module 're' from 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\MITRE_modeling\\\\lib\\\\re.py'>)\n",
      "\u001b[1m\n",
      "File \"<ipython-input-5-c7675d8ca582>\", line 6:\u001b[0m\n",
      "\u001b[1mdef tokenize(behavior, delimiter):\n",
      "    <source elided>\n",
      "\n",
      "\u001b[1m    for match in re.finditer(delimiter, behavior):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of get attribute at <ipython-input-5-c7675d8ca582> (6)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-5-c7675d8ca582>\", line 6:\u001b[0m\n",
      "\u001b[1mdef tokenize(behavior, delimiter):\n",
      "    <source elided>\n",
      "\n",
      "\u001b[1m    for match in re.finditer(delimiter, behavior):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit(parallel = True, fastmath = True)\n",
      "<ipython-input-5-c7675d8ca582>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"tokenize\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-5-c7675d8ca582>\", line 6:\u001b[0m\n",
      "\u001b[1mdef tokenize(behavior, delimiter):\n",
      "    <source elided>\n",
      "\n",
      "\u001b[1m    for match in re.finditer(delimiter, behavior):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit(parallel = True, fastmath = True)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\numba\\core\\object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"tokenize\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-5-c7675d8ca582>\", line 3:\u001b[0m\n",
      "\u001b[1mdef tokenize(behavior, delimiter):\n",
      "\u001b[1m    behavior_vector = [3]\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\numba\\core\\object_mode_passes.py:188: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-5-c7675d8ca582>\", line 3:\u001b[0m\n",
      "\u001b[1mdef tokenize(behavior, delimiter):\n",
      "\u001b[1m    behavior_vector = [3]\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-5-c7675d8ca582>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"tokenize\" failed type inference due to: \u001b[1m\u001b[1mUnknown attribute 'finditer' of type Module(<module 're' from 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\MITRE_modeling\\\\lib\\\\re.py'>)\n",
      "\u001b[1m\n",
      "File \"<ipython-input-5-c7675d8ca582>\", line 6:\u001b[0m\n",
      "\u001b[1mdef tokenize(behavior, delimiter):\n",
      "    <source elided>\n",
      "\n",
      "\u001b[1m    for match in re.finditer(delimiter, behavior):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of get attribute at <ipython-input-5-c7675d8ca582> (6)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-5-c7675d8ca582>\", line 6:\u001b[0m\n",
      "\u001b[1mdef tokenize(behavior, delimiter):\n",
      "    <source elided>\n",
      "\n",
      "\u001b[1m    for match in re.finditer(delimiter, behavior):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit(parallel = True, fastmath = True)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\numba\\core\\object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"tokenize\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-5-c7675d8ca582>\", line 6:\u001b[0m\n",
      "\u001b[1mdef tokenize(behavior, delimiter):\n",
      "    <source elided>\n",
      "\n",
      "\u001b[1m    for match in re.finditer(delimiter, behavior):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\MITRE_modeling\\lib\\site-packages\\numba\\core\\object_mode_passes.py:188: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-5-c7675d8ca582>\", line 6:\u001b[0m\n",
      "\u001b[1mdef tokenize(behavior, delimiter):\n",
      "    <source elided>\n",
      "\n",
      "\u001b[1m    for match in re.finditer(delimiter, behavior):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "full pass: 1912.5839371681213\n",
      "load sample: 0.08399796485900879\n",
      "sort: 0.0\n",
      "strip: 0.0019998550415039062\n",
      "cast: 0.019999980926513672\n",
      "vectorize: 1912.3919377326965\n",
      "avg tokenize: 1.2817573272510763\n",
      "avg append: 4.022594431450156e-06\n",
      "dataset: 0.08300042152404785\n",
      "write: 0.0020012855529785156\n"
     ]
    }
   ],
   "source": [
    "mal_classes = ['adware', 'banking', 'riskware', 'sms']\n",
    "error_hashes = []\n",
    "parent_dir = 'X:\\\\MITRE\\\\MalDroid Data\\\\MalDroid_feature_engineering\\\\'\n",
    "\n",
    "for mal_class in mal_classes:\n",
    "    for sample_folder in os.listdir(parent_dir + mal_class + '\\\\'):\n",
    "        # with open(parent_dir + mal_class + '\\\\' + sample_folder + '\\\\sample_for_analysis.apk.json') as sample_path:\n",
    "        start_sample = time.time()\n",
    "        with open('X:\\\\MITRE\\\\MalDroid Data\\\\MalDroid_feature_engineering\\\\riskware\\\\f6036a5730f8ec961bc1666cfc3269dbf9b854b1a705a65610154a697e804d79\\\\sample_for_analysis.apk.json') as sample_path:\n",
    "            try:\n",
    "                sample_behaviors = json.load(sample_path)['behaviors']['dynamic']['host']\n",
    "            except:\n",
    "                error_hashes.append(sample_folder)\n",
    "                continue\n",
    "        load_end = time.time()\n",
    "\n",
    "        sort_start = time.time()\n",
    "        sorted_behaviors = sort_behaviors(sample_behaviors)\n",
    "        sort_end = time.time()\n",
    "        sample_behaviors = []\n",
    "        # lists are cleared after useage to preserve memory resources\n",
    "\n",
    "        strip_start = time.time()\n",
    "        stripped_behaviors = strip_unused_keys(sorted_behaviors, ['arguments', 'blob', 'parameters', 'id', 'xref', 'ts', 'tid', 'interfaceGroup', 'methodName'])\n",
    "        strip_end = time.time()\n",
    "        sorted_behaviors = []\n",
    "\n",
    "        cast_str_start = time.time()\n",
    "        string_behaviors = [json.dumps(behavior) for behavior in stripped_behaviors] \n",
    "        cast_str_end = time.time()\n",
    "        stripped_behaviors = []\n",
    "\n",
    "        vector_start = time.time()\n",
    "        token_times = []\n",
    "        append_times = []\n",
    "        vectorized_sample = [1]\n",
    "        for behavior in string_behaviors:\n",
    "            token_start = time.time()\n",
    "            append_to_vector = tokenize(behavior, regex_pattern)\n",
    "            token_end = time.time()\n",
    "            token_times.append(token_end-token_start)\n",
    "            append_start = time.time()\n",
    "            for scalar in append_to_vector:\n",
    "                vectorized_sample.append(scalar)\n",
    "            append_end = time.time()\n",
    "            append_times.append(append_end-append_start)\n",
    "        vectorized_sample.append(2)\n",
    "        vector_end = time.time()\n",
    "\n",
    "        ds_start = time.time()\n",
    "        tf_dataset = tf.data.Dataset.from_tensor_slices(vectorized_sample)\n",
    "        tf_dataset = tf_dataset.map(tf.io.serialize_tensor)\n",
    "        ds_end = time.time()\n",
    "\n",
    "        write_start = time.time()\n",
    "        writer = tf.data.experimental.TFRecordWriter(\"X:/MITRE/MITRE_modeling/TFRecord_files/\" + mal_class + \"/\" + sample_folder + \".tfrecord\")\n",
    "        writer.write(tf_dataset)\n",
    "        end_sample = time.time()\n",
    "\n",
    "        print(\"full pass: {0}\".format(end_sample-start_sample))\n",
    "        print(\"load sample: {0}\".format(load_end-start_sample))\n",
    "        print(\"sort: {0}\".format(sort_end-sort_start))\n",
    "        print(\"strip: {0}\".format(strip_end-strip_start))\n",
    "        print(\"cast: {0}\".format(cast_str_end-cast_str_start))\n",
    "        print(\"vectorize: {0}\".format(vector_end-vector_start))\n",
    "        print(\"avg tokenize: {0}\".format(mean(token_times)))\n",
    "        print(\"avg append: {0}\".format(mean(append_times)))\n",
    "        print(\"dataset: {0}\".format(ds_end-ds_start))\n",
    "        print(\"write: {0}\".format(end_sample-write_start))\n",
    "\n",
    "        with open('parallel_vector.txt', 'w') as write_test:\n",
    "            for token in vectorized_sample:\n",
    "                write_test.write('%s\\n' % token)\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}