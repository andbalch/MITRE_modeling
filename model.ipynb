{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd09e45ca6d2f9a632adb471d6a7900c6b597dba7cec4c13cfc3bc8479688782778",
   "display_name": "Python 3.7.7 64-bit ('MITRE_modeling': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model Training and Evaluation\n",
    "This is the final notebook for the training and evaluation of the BERT-like architecture, built on top of TensorFlow examples and tutorials. It will use the training data from the vectorized_samples directory containing 3.1 GB of 5000 .npy fizes of vectorized MalDroid analysis. The samples are broken up into categories as follows:\n",
    "* Adware: 812 (~15.8%)\n",
    "* Banking: 1438 (~28%)\n",
    "* SMS: 1442 (~28.06%)\n",
    "* Riskware: 1447 (~28.16%)\n",
    "## Objectives\n",
    "1. Set up input pipeline to read directly from notebook filesystem\n",
    "2. Implement pipeline optimizations outlined in the TensorFlow docs\n",
    "3. Define the BERT from TensorFlow docs, adding head classifier layer(s)\n",
    "4. Write the training loop \n",
    "5. Implement logging of associated metrics and save checkpoints\n",
    "6. Train and evaluate (be sure to properly initalize weights)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4.1\nNum GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "# confirm tensorflow is using GPU:\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\nYour GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1660 Ti, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "# setup mixed precision for GPU\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# setup mixed precision for TPU\n",
    "# mixed_precision.set_global_policy('mixed_bfloat16')\n",
    "# see https://www.tensorflow.org/guide/mixed_precision#summary for mixed precision guidelines"
   ]
  },
  {
   "source": [
    "## Input pipeline\n",
    "The pipeline needs to meet the following criteria:\n",
    "* Avoid loading the whole dataset into memory\n",
    "* Apply padding to the samples (max_len: 2783755, trimmed to 5039: 1283945)\n",
    "* Implement a fix for unbalanced data\n",
    "* Batch the samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max length of samples\n",
    "\n",
    "# max_sample_len = 0\n",
    "# for mal_class in os.listdir('vectorized_samples'):\n",
    "#     parent_path = 'vectorized_samples/' + mal_class + '/'\n",
    "#     for sample_path in os.listdir(parent_path):\n",
    "#         len_list.append(np.load(parent_path + sample_path).size)\n",
    "#         if sample_len > max_sample_len:\n",
    "#             max_sample_len = sample_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of sample lengths for analysis\n",
    "\n",
    "# len_list = []\n",
    "# for mal_class in os.listdir('vectorized_samples'):\n",
    "#     parent_path = 'vectorized_samples/' + mal_class + '/'\n",
    "#     for sample_path in os.listdir(parent_path):\n",
    "#         len_list.append(np.load(parent_path + sample_path).size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot = sns.boxplot(x=len_list)"
   ]
  },
  {
   "source": [
    "Boxplot of sample lengths reveals a large amount of outliers, it would be benificial to model performance to limit the max length of the dataset. We have 5,139 samples therefore we could cut the 100 or so greatest lengths. The results of this process shown below yield a max sample length of 1283945, a significant decrease from 2.78 million. This is a viable solution if training cost proves unmanageable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_list.sort(reverse=True)\n",
    "# len_list = len_list[100:]\n",
    "# max_sample_len_trimmed = len_list[0]\n",
    "# print(max_sample_len_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path_list = glob('vectorized_samples/*/*.npy')\n",
    "\n",
    "# shuffling the samples now so the runtime does not have to deal with maintaining a large buffer\n",
    "random.shuffle(sample_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts labels to categorical ints as follows:\n",
    "# adware: 0\n",
    "# banking: 1\n",
    "# sms: 2\n",
    "# riskware: 3\n",
    "\n",
    "def process_path(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        label = tf.strings.split(path, os.path.sep)[-2]\n",
    "        if label == 'adware':\n",
    "            label = 0\n",
    "        elif label == 'banking':\n",
    "            label = 1\n",
    "        elif label == 'sms':\n",
    "            label = 2\n",
    "        elif label == 'riskware':\n",
    "            label = 3\n",
    "        sample = np.load(file_path)\n",
    "        yield sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates a dataset that does not account for class imbalance\n",
    "\n",
    "# data = tf.data.Dataset.from_generator(process_path, args=[sample_path_list], output_types=(tf.int32, tf.int32), output_shapes=((None,), ()))"
   ]
  },
  {
   "source": [
    "### Class Imbalance\n",
    "Analysis of vectorized samples shows that Adware is signifigantly underrepresented, making up around 18% compared to around 28% from each other class. To account for this, we will oversample Adware. Some more advanced techniques like class weighting could be more effective but cannot be implemented due to time constraints and potential training overhead."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not the most efficient but it is useable and is only run once\n",
    "\n",
    "adware_paths = []\n",
    "banking_paths = []\n",
    "sms_paths = []\n",
    "riskware_paths = []\n",
    "\n",
    "for path in sample_path_list:\n",
    "    label = tf.strings.split(path, os.path.sep)[-2]\n",
    "    if label == 'adware':\n",
    "        adware_paths.append(path)\n",
    "    elif label == 'banking':\n",
    "        banking_paths.append(path)\n",
    "    elif label == 'sms':\n",
    "        sms_paths.append(path)\n",
    "    elif label == 'riskware':\n",
    "        riskware_paths.append(path)\n",
    "\n",
    "# cleaning up sample_path_list\n",
    "\n",
    "sample_path_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adware_data = tf.data.Dataset.from_generator(process_path, args=[adware_paths], output_types=(tf.int32, tf.int32), output_shapes=((None,), ()))\n",
    "\n",
    "banking_data = tf.data.Dataset.from_generator(process_path, args=[banking_paths], output_types=(tf.int32, tf.int32), output_shapes=((None,), ()))\n",
    "\n",
    "sms_data = tf.data.Dataset.from_generator(process_path, args=[sms_paths], output_types=(tf.int32, tf.int32), output_shapes=((None,), ()))\n",
    "\n",
    "riskware_data = tf.data.Dataset.from_generator(process_path, args=[riskware_paths], output_types=(tf.int32, tf.int32), output_shapes=((None,), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't panic, seed is for reproducible results\n",
    "oversamp_data = tf.data.experimental.sample_from_datasets([adware_data, banking_data, sms_data, riskware_data], weights=[0.25,0.25,0.25,0.25], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 250\n",
    "BATCH_SIZE = 32 # Reccomended by BERT paper, alt is 16\n",
    "DATASET_SIZE = 5139\n",
    "PAD_SIZE = \n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "def make_batches(dataset):\n",
    "    return (\n",
    "        dataset\n",
    "        .cache() # comment out if too memory intensive\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .repeat()\n",
    "        .padded_batch(BATCH_SIZE, padding_values=-1, drop_remainder=True)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "oversamp_data.shuffle(BUFFER_SIZE, seed=42, reshuffle_each_iteration=False)\n",
    "train_dataset = oversamp_data.take(train_size)\n",
    "test_dataset = oversamp_data.skip(train_size)\n",
    "val_dataset = test_dataset.skip(val_size)\n",
    "test_dataset = test_dataset.take(test_size)\n",
    "\n",
    "train_dataset = make_batches(train_dataset)\n",
    "test_dataset = make_batches(test_dataset)\n",
    "val_dataset = make_batches(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1096410), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1285807), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1238131), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1192571), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1071306), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " ...\n",
      " [  1   3  10 ... 576   4   2]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]], shape=(32, 1749028), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ... 576   4   2]\n",
      " ...\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]], shape=(32, 1280152), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1306281), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1306451), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1199571), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1225961), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1080862), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ... 576   4   2]\n",
      " ...\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]], shape=(32, 1492433), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1116138), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ... 576   4   2]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " ...\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]], shape=(32, 245314), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1192289), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " ...\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ...  -1  -1  -1]\n",
      " [  1   3  10 ... 576   4   2]], shape=(32, 1321518), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 719426), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 1190533), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]\n",
      " [ 1  3 10 ... -1 -1 -1]], shape=(32, 2343498), dtype=int32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for sample, label in train_dataset.take(20):\n",
    "    print(sample)\n",
    "    print(label)"
   ]
  },
  {
   "source": [
    "Portions of this page are reproduced from and/or modifications based on work created and shared by Google (https://developers.google.com/readme/policies) and used according to terms described in the Creative Commons 4.0 Attribution License (https://creativecommons.org/licenses/by/4.0/)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}