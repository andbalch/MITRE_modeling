{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd09e45ca6d2f9a632adb471d6a7900c6b597dba7cec4c13cfc3bc8479688782778",
   "display_name": "Python 3.7.7 64-bit ('MITRE_modeling': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model Training and Evaluation\n",
    "This is the final notebook for the training and evaluation of the BERT-like architecture, built on top of TensorFlow examples and tutorials. It will use the training data from the vectorized_samples directory containing 3.1 GB of 5000 .npy fizes of vectorized MalDroid analysis. The samples are broken up into categories as follows:\n",
    "* Adware: 812\n",
    "* Banking: 1438\n",
    "* SMS: 1442\n",
    "* Riskware: 1447\n",
    "## Objectives\n",
    "1. Set up input pipeline to read directly from notebook filesystem\n",
    "2. Implement pipeline optimizations outlined in the TensorFlow docs\n",
    "3. Define the BERT from TensorFlow docs, adding head classifier layer(s)\n",
    "4. Write the training loop \n",
    "5. Implement logging of associated metrics\n",
    "6. Train and evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "# confirm tensorflow is using GPU:\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup mixed precision for GPU\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "# setup mixed precision for TPU\n",
    "# mixed_precision.set_global_policy('mixed_bfloat16')\n",
    "# see https://www.tensorflow.org/guide/mixed_precision#summary for mixed precision guidelines"
   ]
  },
  {
   "source": [
    "## Input pipeline\n",
    "The pipeline needs to meet the following criteria:\n",
    "* Avoid loading the whole dataset into memory\n",
    "* Apply padding to the samples (max_len: 2783755, trimmed to 5039: 1283945)\n",
    "* Implement a fix for unbalanced data\n",
    "* Batch the samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max length of samples\n",
    "# max_sample_len = 0\n",
    "# for mal_class in os.listdir('vectorized_samples'):\n",
    "#     parent_path = 'vectorized_samples/' + mal_class + '/'\n",
    "#     for sample_path in os.listdir(parent_path):\n",
    "#         len_list.append(np.load(parent_path + sample_path).size)\n",
    "#         if sample_len > max_sample_len:\n",
    "#             max_sample_len = sample_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of sample lengths for analysis\n",
    "# len_list = []\n",
    "# for mal_class in os.listdir('vectorized_samples'):\n",
    "#     parent_path = 'vectorized_samples/' + mal_class + '/'\n",
    "#     for sample_path in os.listdir(parent_path):\n",
    "#         len_list.append(np.load(parent_path + sample_path).size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot = sns.boxplot(x=len_list)"
   ]
  },
  {
   "source": [
    "Boxplot of sample lengths reveals a large amount of outliers, it would be benificial to model performance to limit the max length of the dataset. We have 5,139 samples therefore we could cut the 100 or so greatest lengths. The results of this process shown below yield a max sample length of 1283945, a significant decrease from 2.78 million. This is a viable solution if training cost proves unmanageable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_list.sort(reverse=True)\n",
    "# len_list = len_list[100:]\n",
    "# max_sample_len_trimmed = len_list[0]\n",
    "# print(max_sample_len_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path_list = glob('vectorized_samples/*/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        label = tf.strings.split(file_path, os.path.sep)[-2]\n",
    "        sample = np.load(file_path)\n",
    "        yield sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_generator(process_path, args=[sample_path_list], output_types=(tf.int32, tf.string), output_shapes=((None,), ()))"
   ]
  },
  {
   "source": [
    "Portions of this page are reproduced from and/or modifications based on work created and shared by Google (https://developers.google.com/readme/policies) and used according to terms described in the Creative Commons 4.0 Attribution License (https://creativecommons.org/licenses/by/4.0/)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}